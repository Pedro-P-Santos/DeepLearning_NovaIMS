{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip \"/content/drive/MyDrive/deep_learning_project/rare_species 1.zip\" -d \"/content/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ConvNeXtBase\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_dir_drive = \"/content/drive/MyDrive/deep_learning_project\"\n",
    "image_dir = \"/content/rare_species 1\"\n",
    "\n",
    "df = pd.read_csv(os.path.join(base_dir_drive, \"metadata.csv\"))\n",
    "df[\"file_path\"] = df[\"file_path\"].str.replace(\"\\\\\", \"/\", regex=False)\n",
    "df[\"file_path\"] = df[\"file_path\"].apply(lambda x: os.path.join(image_dir, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"file_path\"]  # Caminhos das imagens\n",
    "y = df[\"family\"]  # Classes correspondentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(y.unique())\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "train_data = pd.DataFrame({\"file_path\": X_train, \"family\": y_train})\n",
    "test_data = pd.DataFrame({\"file_path\": X_test, \"family\": y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_data[\"label\"] = label_encoder.fit_transform(train_data[\"family\"])\n",
    "test_data[\"label\"] = label_encoder.transform(test_data[\"family\"])  # usar o mesmo encoder\n",
    "n_classes = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def load_image(path, label):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # normalização\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# rotation = tf.keras.layers.RandomRotation(0.1)\n",
    "# zoom = tf.keras.layers.RandomZoom(0.1)\n",
    "\n",
    "rotation = tf.keras.layers.RandomRotation(0.2)\n",
    "zoom = tf.keras.layers.RandomZoom(0.2)\n",
    "\n",
    "def augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "    image = rotation(image)\n",
    "    image = zoom(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_HEAD = 32\n",
    "BATCH_SIZE_FINE = 64\n",
    "#BATCH_SIZE_TESTAR = 96\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# 1. Pipeline base único (sem batch ainda)\n",
    "base_train_ds = tf.data.Dataset.from_tensor_slices((train_data[\"file_path\"].values, train_data[\"label\"].values))\n",
    "base_train_ds = base_train_ds.map(load_image, num_parallel_calls=AUTOTUNE)\n",
    "base_train_ds = base_train_ds.map(augment, num_parallel_calls=AUTOTUNE)\n",
    "base_train_ds = base_train_ds.shuffle(buffer_size=1000).prefetch(AUTOTUNE)\n",
    "\n",
    "base_val_ds = tf.data.Dataset.from_tensor_slices((test_data[\"file_path\"].values, test_data[\"label\"].values))\n",
    "base_val_ds = base_val_ds.map(load_image, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
    "\n",
    "# 2. Versões batched\n",
    "train_ds = base_train_ds.batch(BATCH_SIZE_HEAD)\n",
    "val_ds = base_val_ds.batch(BATCH_SIZE_HEAD)\n",
    "\n",
    "train_ds_ft = base_train_ds.batch(BATCH_SIZE_FINE)\n",
    "val_ds_ft = base_val_ds.batch(BATCH_SIZE_FINE)\n",
    "\n",
    "# train_ds_testar = base_train_ds.batch(BATCH_SIZE_TESTAR)\n",
    "# val_ds_testar = base_val_ds.batch(BATCH_SIZE_TESTAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.join(base_dir_drive, \"modelos\")\n",
    "\n",
    "callbacks_finetune_model2 = [\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, \"best_model2_effnetv2_finetune.keras\"),\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "callbacks_head_model2 = [\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, \"best_model2_effnetv2_head.keras\"),\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ConvNeXtBase\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# 1. Modelo base\n",
    "base_model2 = ConvNeXtBase(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "base_model2.trainable = False  # congelado no início\n",
    "\n",
    "# 2. Construir modelo com head agressivamente regularizada\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "x = base_model2(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(1024, activation=\"gelu\", kernel_regularizer=l2(0.005))(x)\n",
    "x = Dropout(0.6)(x)\n",
    "x = Dense(512, activation=\"gelu\", kernel_regularizer=l2(0.005))(x)\n",
    "x = Dropout(0.6)(x)\n",
    "output = Dense(n_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model2 = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# 3. Compilar (AdamW + regularização agressiva)\n",
    "model2.compile(\n",
    "    optimizer=AdamW(learning_rate=1e-4, weight_decay=3e-4),\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# 4. Treinar head com callbacks já definidos\n",
    "history_model2_head = model2.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=callbacks_head_model2  # já definidos\n",
    ")\n",
    "\n",
    "# 5. Descongelar últimas 200 camadas para fine-tuning\n",
    "for layer in base_model2.layers[-300:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# 6. Recompilar para fine-tuning\n",
    "model2.compile(\n",
    "    optimizer=AdamW(learning_rate=5e-6, weight_decay=3e-4),\n",
    "    loss=SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# 7. Treinar com fine-tuning\n",
    "history_model2_finetuned = model2.fit(\n",
    "    train_ds_ft,\n",
    "    validation_data=val_ds_ft,\n",
    "    epochs=55,\n",
    "    callbacks=callbacks_finetune_model2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
