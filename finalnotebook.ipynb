{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "\n",
    "# model building imports\n",
    "from keras import Model, Sequential, Input\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.ops import add\n",
    "from keras.utils import to_categorical\n",
    "# model training imports\n",
    "from keras.optimizers import SGD\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.metrics import CategoricalAccuracy, AUC, F1Score\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
    "\n",
    "\n",
    "from keras.layers import RandomContrast, RandomSharpness\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom\n",
    "from tensorflow.keras.layers import RandAugment\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Import version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Caminho base onde estão as pastas com imagens\n",
    "# base_image_dir = 'C:/Users/luisp/OneDrive/Mestrado/1ano/2sem/Deep Learning (DL)/Project/rare_species 1'\n",
    "\n",
    "# # Criar coluna com caminho absoluto para cada imagem\n",
    "# metadata['absolute_path'] = metadata['file_path'].apply(lambda x: os.path.join(base_image_dir, x))\n",
    "\n",
    "# # Verificar se o ficheiro existe no disco\n",
    "# metadata['exists_on_disk'] = metadata['absolute_path'].apply(os.path.exists)\n",
    "\n",
    "# # Mostrar quantos existem e quantos faltam\n",
    "# total_in_metadata = len(metadata)\n",
    "# existing_on_disk = metadata['exists_on_disk'].sum()\n",
    "# real_images_on_disk = sum(len(files) for _, _, files in os.walk(base_image_dir))\n",
    "\n",
    "# print(f\"Total de imagens no metadata: {total_in_metadata}\")\n",
    "# print(f\"Total de imagens que existem no disco: {existing_on_disk}\")\n",
    "# print(f\"Total real de ficheiros de imagem no disco: {real_images_on_disk}\")\n",
    "\n",
    "# # Mostrar exemplos de imagens em falta\n",
    "# print(\"\\nExemplos de imagens em falta:\")\n",
    "# print(metadata[~metadata['exists_on_disk']][['file_path']].head())\n",
    "\n",
    "# # Remover entradas com ficheiros em falta\n",
    "# metadata_clean = metadata[metadata['exists_on_disk']].drop(columns=['exists_on_disk'])\n",
    "\n",
    "# # Guardar novo ficheiro limpo\n",
    "# metadata_clean.to_csv(\"C:/Users/luisp/OneDrive/Mestrado/1ano/2sem/Deep Learning (DL)/Project/rare_species 1/metadata_clean.csv\", index=False)\n",
    "\n",
    "# print(\"\\n✅ metadata_clean.csv guardado com sucesso — apenas com imagens existentes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Import 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is comented because this split was already done in our pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata = pd.read_csv(r\"/Users/pedrosantos/Documents 2/Deep Learning/Projeto/rare_species 1/metadata.csv\")\n",
    "# metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata['family'] = metadata['family'].astype(str)  # Ensure it's a string\n",
    "# metadata['file_path'] = metadata['file_path'].astype(str)  # Ensure path is string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define output directories\n",
    "# output_base = r\"/Users/pedrosantos/Documents 2/Deep Learning/Projeto/rare_species 1/dataset_split\"\n",
    "# os.makedirs(output_base, exist_ok=True)\n",
    "# for split in ['train', 'val', 'test']:\n",
    "#     os.makedirs(os.path.join(output_base, split), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stratified split at image level\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_files, temp_files = train_test_split(metadata, test_size=0.3, stratify=metadata['family'], random_state=42)\n",
    "# val_files, test_files = train_test_split(temp_files, test_size=0.5, stratify=temp_files['family'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def move_files(file_subset, split_name):\n",
    "#     for _, row in file_subset.iterrows():\n",
    "#         src_path = os.path.join(dataset_directory, row['file_path'])  # Construct full path\n",
    "#         dst_path = os.path.join(output_base, split_name, row['file_path'])\n",
    "#         os.makedirs(os.path.dirname(dst_path), exist_ok=True)  # Ensure folder structure\n",
    "#         if os.path.exists(src_path):\n",
    "#             shutil.copy2(src_path, dst_path)\n",
    "#         else:\n",
    "#             print(f\"Warning: {src_path} not found.\")\n",
    "\n",
    "# # Move images to respective folders\n",
    "# move_files(train_files, 'train')\n",
    "# move_files(val_files, 'val')\n",
    "# move_files(test_files, 'test')\n",
    "\n",
    "# print(\"Dataset split completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pedro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r\"/Users/pedrosantos/Documents 2/Deep Learning/Projeto/rare_species 1/dataset_split/train\"\n",
    "valid_path = r\"/Users/pedrosantos/Documents 2/Deep Learning/Projeto/rare_species 1/dataset_split/val\"\n",
    "test_path = r\"/Users/pedrosantos/Documents 2/Deep Learning/Projeto/rare_species 1/dataset_split/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r\"C:\\Users\\inesm\\OneDrive\\Documentos\\Mestrado\\1º ano\\2ºS\\DeepLearning\\Project DL\\dataset_split\\train\"\n",
    "valid_path = r\"C:\\Users\\inesm\\OneDrive\\Documentos\\Mestrado\\1º ano\\2ºS\\DeepLearning\\Project DL\\dataset_split\\val\"\n",
    "test_path = r\"C:\\Users\\inesm\\OneDrive\\Documentos\\Mestrado\\1º ano\\2ºS\\DeepLearning\\Project DL\\dataset_split\\test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xba in position 49: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === 3. Image Data Generators ===\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_ds = \u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcategorical\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m val_ds = image_dataset_from_directory(\n\u001b[32m     10\u001b[39m     valid_path,\n\u001b[32m     11\u001b[39m     label_mode=\u001b[33m'\u001b[39m\u001b[33mcategorical\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     batch_size=BATCH_SIZE\n\u001b[32m     15\u001b[39m )\n\u001b[32m     16\u001b[39m test_ds = image_dataset_from_directory(\n\u001b[32m     17\u001b[39m     test_path,\n\u001b[32m     18\u001b[39m     label_mode=\u001b[33m'\u001b[39m\u001b[33mcategorical\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     23\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\inesm\\anaconda3\\envs\\dl\\Lib\\site-packages\\keras\\src\\utils\\image_dataset_utils.py:232\u001b[39m, in \u001b[36mimage_dataset_from_directory\u001b[39m\u001b[34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    231\u001b[39m     seed = np.random.randint(\u001b[32m1e6\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m image_paths, labels, class_names = \u001b[43mdataset_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mALLOWLIST_FORMATS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m label_mode == \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) != \u001b[32m2\u001b[39m:\n\u001b[32m    244\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    245\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mWhen passing `label_mode=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`, there must be exactly 2 \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    247\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\inesm\\anaconda3\\envs\\dl\\Lib\\site-packages\\keras\\src\\utils\\dataset_utils.py:530\u001b[39m, in \u001b[36mindex_directory\u001b[39m\u001b[34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[39m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels == \u001b[33m\"\u001b[39m\u001b[33minferred\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    529\u001b[39m     subdirs = []\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[32m    531\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m tf.io.gfile.isdir(tf.io.gfile.join(directory, subdir)):\n\u001b[32m    532\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m subdir.startswith(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\inesm\\anaconda3\\envs\\dl\\Lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:767\u001b[39m, in \u001b[36mlist_directory_v2\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    751\u001b[39m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mio.gfile.listdir\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlist_directory_v2\u001b[39m(path):\n\u001b[32m    753\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[32m    754\u001b[39m \n\u001b[32m    755\u001b[39m \u001b[33;03m  The list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    765\u001b[39m \u001b[33;03m    errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[32m    766\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    768\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m errors.NotFoundError(\n\u001b[32m    769\u001b[39m         node_def=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    770\u001b[39m         op=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    771\u001b[39m         message=\u001b[33m\"\u001b[39m\u001b[33mCould not find directory \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(path))\n\u001b[32m    773\u001b[39m   \u001b[38;5;66;03m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[32m    774\u001b[39m   \u001b[38;5;66;03m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\inesm\\anaconda3\\envs\\dl\\Lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:689\u001b[39m, in \u001b[36mis_directory\u001b[39m\u001b[34m(dirname)\u001b[39m\n\u001b[32m    679\u001b[39m \u001b[38;5;129m@tf_export\u001b[39m(v1=[\u001b[33m\"\u001b[39m\u001b[33mgfile.IsDirectory\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_directory\u001b[39m(dirname):\n\u001b[32m    681\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Returns whether the path is a directory or not.\u001b[39;00m\n\u001b[32m    682\u001b[39m \n\u001b[32m    683\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    687\u001b[39m \u001b[33;03m    True, if the path is a directory; False otherwise\u001b[39;00m\n\u001b[32m    688\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mis_directory_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\inesm\\anaconda3\\envs\\dl\\Lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:703\u001b[39m, in \u001b[36mis_directory_v2\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    694\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns whether the path is a directory or not.\u001b[39;00m\n\u001b[32m    695\u001b[39m \n\u001b[32m    696\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    700\u001b[39m \u001b[33;03m  True, if the path is a directory; False otherwise\u001b[39;00m\n\u001b[32m    701\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pywrap_file_io\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIsDirectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m errors.OpError:\n\u001b[32m    705\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xba in position 49: invalid start byte"
     ]
    }
   ],
   "source": [
    "# === 3. Image Data Generators ===\n",
    "train_ds = image_dataset_from_directory(\n",
    "    train_path,\n",
    "    label_mode='categorical',\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "val_ds = image_dataset_from_directory(\n",
    "    valid_path,\n",
    "    label_mode='categorical',\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "test_ds = image_dataset_from_directory(\n",
    "    test_path,\n",
    "    label_mode='categorical',\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
